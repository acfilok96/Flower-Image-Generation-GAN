{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5UV5Im06Qg7"
   },
   "source": [
    "## Environment set up and import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:46.769179Z",
     "iopub.status.busy": "2022-08-25T18:59:46.768840Z",
     "iopub.status.idle": "2022-08-25T18:59:46.792249Z",
     "shell.execute_reply": "2022-08-25T18:59:46.791488Z",
     "shell.execute_reply.started": "2022-08-25T18:59:46.769084Z"
    },
    "id": "b-0LJOu1Jumr",
    "outputId": "7fc4ad26-aae2-42e8-d56a-92e88a0c2e57"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:46.794381Z",
     "iopub.status.busy": "2022-08-25T18:59:46.794027Z",
     "iopub.status.idle": "2022-08-25T18:59:54.453513Z",
     "shell.execute_reply": "2022-08-25T18:59:54.451526Z",
     "shell.execute_reply.started": "2022-08-25T18:59:46.794344Z"
    },
    "id": "izI8Tcx5Sswy",
    "outputId": "a6446b8f-5e65-474b-bf91-62fc303df26b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "with tf.device(device_name):\n",
    "  print(device_name.split(\":\")[1],\" running . . . \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:54.455450Z",
     "iopub.status.busy": "2022-08-25T18:59:54.455187Z",
     "iopub.status.idle": "2022-08-25T18:59:54.461393Z",
     "shell.execute_reply": "2022-08-25T18:59:54.460549Z",
     "shell.execute_reply.started": "2022-08-25T18:59:54.455410Z"
    },
    "id": "BZNKon484KfJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:54.463354Z",
     "iopub.status.busy": "2022-08-25T18:59:54.462628Z",
     "iopub.status.idle": "2022-08-25T18:59:55.902169Z",
     "shell.execute_reply": "2022-08-25T18:59:55.901403Z",
     "shell.execute_reply.started": "2022-08-25T18:59:54.463296Z"
    },
    "id": "U7Vkit2dTCvS"
   },
   "outputs": [],
   "source": [
    "import os, keras, numpy,tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import *\n",
    "from numpy.random import *\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1V1zVhkQ6V1P"
   },
   "source": [
    "## **Discriminator** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:55.906889Z",
     "iopub.status.busy": "2022-08-25T18:59:55.906673Z",
     "iopub.status.idle": "2022-08-25T18:59:56.455604Z",
     "shell.execute_reply": "2022-08-25T18:59:56.454887Z",
     "shell.execute_reply.started": "2022-08-25T18:59:55.906864Z"
    },
    "id": "13b25edNTGV5",
    "outputId": "c347d8ac-90f5-4c74-fc47-00fc968ac618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_label (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 150)       750         ['input_label[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1, 4096)      618496      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " input_image (InputLayer)       [(None, 64, 64, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 64, 64, 1)    0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64, 64, 4)    0           ['input_image[0][0]',            \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 150)       750         ['input_label[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 64)   2368        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1, 1024)      154624      ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 32, 32, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 32, 32, 1)    0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 65)   0           ['leaky_re_lu[0][0]',            \n",
      "                                                                  'reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 16, 16, 128)  75008       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 16, 16, 128)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 8, 8, 128)    147584      ['leaky_re_lu_1[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 8, 8, 128)    0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 4, 4, 128)    147584      ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 4, 4, 128)    0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['leaky_re_lu_3[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 2048)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2048)         4196352     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            2049        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,345,565\n",
      "Trainable params: 5,345,565\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_discriminator(in_shape=(64,64,3),n_classes=5):\n",
    "\n",
    "    # input level\n",
    "    label_layer_1 = Input(shape=(1,), name = \"input_label\")\n",
    "    label_layer_2 = Embedding(n_classes, 150)(label_layer_1)\n",
    "    label_layer_3 = Dense(in_shape[0] * in_shape[1])(label_layer_2)\n",
    "    label_layer_4 = Reshape((in_shape[0], in_shape[1], 1))(label_layer_3)\n",
    "    # (64, 64, 1)\n",
    "\n",
    "    # input image\n",
    "    input_image = Input(shape=in_shape, name = \"input_image\")\n",
    "    # (64, 64, 3)\n",
    "\n",
    "    concat_layer = Concatenate()([input_image, label_layer_4])\n",
    "    # (64, 64, 4)\n",
    "\n",
    "    conv2d_layer_2 = Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), padding = 'same')(concat_layer)\n",
    "    conv2d_layer_2 = LeakyReLU(alpha=0.4)(conv2d_layer_2)\n",
    "    # (32, 32, 64)\n",
    "\n",
    "    label_layer_2_1 = Embedding(n_classes, 150)(label_layer_1)\n",
    "    label_layer_3_1 = Dense(32 * 32)(label_layer_2_1)\n",
    "    label_layer_4_1 = Reshape((32, 32, 1))(label_layer_3_1)\n",
    "    # (32, 32, 1)\n",
    "\n",
    "    concat_layer_1 = Concatenate()([conv2d_layer_2, label_layer_4_1])\n",
    "    # (32, 32, 65)\n",
    "\n",
    "    conv2d_layer_3 = Conv2D(filters = 128, kernel_size = (3,3),  strides = (2,2), padding = 'same')(concat_layer_1)\n",
    "    conv2d_layer_3 = LeakyReLU(alpha=0.4)(conv2d_layer_3)\n",
    "    # (16, 16, 128)\n",
    "\n",
    "    conv2d_layer_4 = Conv2D(filters = 128, kernel_size = (3,3), strides = (2,2), padding = 'same')(conv2d_layer_3)\n",
    "    conv2d_layer_4 = LeakyReLU(alpha=0.4)(conv2d_layer_4)\n",
    "    # (8, 8, 128)\n",
    "\n",
    "    conv2d_layer_5 = Conv2D(filters = 128, kernel_size = (3,3), strides = (2,2), padding = 'same')(conv2d_layer_4)\n",
    "    conv2d_layer_5 = LeakyReLU(alpha=0.4)(conv2d_layer_5)\n",
    "    # (4, 4, 128)\n",
    "\n",
    "    flatten_layer = Flatten()(conv2d_layer_5)\n",
    "    # (4 * 4 * 128)\n",
    "\n",
    "    dropout_layer = Dropout(rate=0.4)(flatten_layer)\n",
    "    # (4 * 4 * 128)\n",
    "\n",
    "    dense_layer = Dense(128 * 8 * 2, activation='relu')(dropout_layer)\n",
    "    # (8 * 128 * 2)\n",
    "\n",
    "    # final layer\n",
    "    output_layer = Dense(1, activation='linear')(dense_layer)\n",
    "    # (1,)\n",
    "\n",
    "    model = Model([input_image,label_layer_1], output_layer)\n",
    "\n",
    "    opt = Adam(learning_rate= 3e-4, beta_1=0.5)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "a = define_discriminator()\n",
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:56.459104Z",
     "iopub.status.busy": "2022-08-25T18:59:56.458870Z",
     "iopub.status.idle": "2022-08-25T18:59:57.622990Z",
     "shell.execute_reply": "2022-08-25T18:59:57.622193Z",
     "shell.execute_reply.started": "2022-08-25T18:59:56.459075Z"
    },
    "id": "f4D-H0p65xNd",
    "outputId": "7873da65-5c5a-4902-9bcb-8c638573ff0a"
   },
   "outputs": [],
   "source": [
    "# plot the discriminator model\n",
    "tf.keras.utils.plot_model(a,to_file='discriminator.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "As6FQBwl6jjG"
   },
   "source": [
    "## **Generator** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:57.624709Z",
     "iopub.status.busy": "2022-08-25T18:59:57.624481Z",
     "iopub.status.idle": "2022-08-25T18:59:57.864906Z",
     "shell.execute_reply": "2022-08-25T18:59:57.864033Z",
     "shell.execute_reply.started": "2022-08-25T18:59:57.624679Z"
    },
    "id": "0GfMsg2WTJUK",
    "outputId": "bad411c1-4ec9-4b0d-de76-fec6dd9396a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_latent (InputLayer)      [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_label (InputLayer)       [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 8192)         4202496     ['input_latent[0][0]']           \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 150)       750         ['input_label[0][0]']            \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 8192)         0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1, 64)        9664        ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)            (None, 8, 8, 128)    0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)            (None, 8, 8, 1)      0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8, 8, 129)    0           ['reshape_3[0][0]',              \n",
      "                                                                  'reshape_2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTransp  (None, 16, 16, 64)  74368       ['concatenate_2[0][0]']          \n",
      " ose)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 16, 16, 64)   36928       ['conv2d_transpose[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 16, 16, 64)  256         ['conv2d_4[0][0]']               \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1, 256)       38656       ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 16, 16, 64)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)            (None, 16, 16, 1)    0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 16, 16, 65)   0           ['leaky_re_lu_4[0][0]',          \n",
      "                                                                  'reshape_4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2DTran  (None, 32, 32, 64)  37504       ['concatenate_3[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 64)   36928       ['conv2d_transpose_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 32, 32, 64)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 64)   36928       ['leaky_re_lu_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 32, 32, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 32, 32, 128)  0           ['leaky_re_lu_5[0][0]',          \n",
      "                                                                  'leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 64, 64, 64)  73792       ['concatenate_4[0][0]']          \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 64, 64, 64)   36928       ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 64, 64)  256         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 64, 64, 64)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 64, 64, 64)   36928       ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 64, 64)  256         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 64, 64, 64)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 64, 64, 128)  0           ['leaky_re_lu_7[0][0]',          \n",
      "                                                                  'leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 64, 64, 3)    6147        ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,629,297\n",
      "Trainable params: 4,628,657\n",
      "Non-trainable params: 640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def define_generator(latent_dim = 100, n_classes = 5):\n",
    "\n",
    "    # input level\n",
    "    label_layer_1 = Input(shape=(1,), name = \"input_label\")\n",
    "    label_layer_2 = Embedding(n_classes, 150)(label_layer_1)\n",
    "    label_layer_3 = Dense(8 * 8)(label_layer_2)\n",
    "    label_layer_4 = Reshape((8, 8, 1))(label_layer_3)\n",
    "    # (8, 8, 1)\n",
    "\n",
    "    # latent input\n",
    "    latent_layer = Input(shape=(latent_dim,), name = \"input_latent\")\n",
    "\n",
    "    layer_2 = Dense(128 * 8 * 8)(latent_layer)\n",
    "    layer_2 = Activation(\"relu\")(layer_2)\n",
    "    layer_2 = Reshape((8, 8, 128))(layer_2)\n",
    "    # (8, 8, 128)\n",
    "\n",
    "\n",
    "    concat_layer_1 = Concatenate()([layer_2, label_layer_4])\n",
    "    # (8, 8, 129)\n",
    "\n",
    "    layer_3 = Conv2DTranspose(filters = 64, kernel_size = (3,3), strides=(2,2), padding='same')(concat_layer_1)\n",
    "    layer_3 = Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_initializer = initializers.RandomNormal(0,0.8))(layer_3)\n",
    "    layer_3 = BatchNormalization(momentum = 0.8)(layer_3)\n",
    "    layer_3 = LeakyReLU(alpha=0.4)(layer_3)\n",
    "    # (16, 16, 64)\n",
    "    \n",
    "    label_layer_3_1 = Dense(16 * 16)(label_layer_2)\n",
    "    label_layer_4_1 = Reshape((16, 16, 1))(label_layer_3_1)\n",
    "    # 16, 16, 1)\n",
    "\n",
    "    concat_layer_2 = Concatenate()([layer_3, label_layer_4_1])\n",
    "    # (16, 16, 65)\n",
    "\n",
    "    layer_4 = Conv2DTranspose(filters = 64, kernel_size = (3,3), strides=(2,2), padding='same')(concat_layer_2)\n",
    "    layer_4 = Conv2D(filters = 64, kernel_size = (3,3), padding='same',kernel_initializer = initializers.RandomNormal(0,0.8))(layer_4)\n",
    "    layer_4 = BatchNormalization(momentum = 0.8)(layer_4)\n",
    "    layer_4 = LeakyReLU(alpha=0.4)(layer_4)\n",
    "    # (32, 32, 64)\n",
    "    \n",
    "    layer_4_1 = Conv2D(filters = 64, kernel_size = (3,3), padding='same',kernel_initializer = initializers.RandomNormal(0,0.8))(layer_4)\n",
    "    layer_4_1 = BatchNormalization(momentum = 0.8)(layer_4_1)\n",
    "    layer_4_1 = LeakyReLU(alpha=0.4)(layer_4_1)\n",
    "    # (32, 32, 64)\n",
    "\n",
    "    concat_layer_2 = Concatenate()([layer_4, layer_4_1])\n",
    "    # (32, 32, 128)\n",
    "\n",
    "\n",
    "    layer_5 = Conv2DTranspose(filters = 64, kernel_size = (3,3), strides=(2,2), padding='same')(concat_layer_2)\n",
    "    layer_5 = Conv2D(filters = 64, kernel_size = (3,3), padding='same',kernel_initializer = initializers.RandomNormal(0,0.8))(layer_5)\n",
    "    layer_5 = BatchNormalization(momentum = 0.8)(layer_5)\n",
    "    layer_5 = LeakyReLU(alpha=0.4)(layer_5)\n",
    "    # (64, 64, 64)\n",
    "\n",
    "    layer_6 = Conv2D(filters = 64, kernel_size = (3,3), padding='same',kernel_initializer = initializers.RandomNormal(0,0.8))(layer_5)\n",
    "    layer_6 = BatchNormalization(momentum = 0.8)(layer_6)\n",
    "    layer_6 = LeakyReLU(alpha=0.4)(layer_6)\n",
    "    # (64, 64, 64)\n",
    "\n",
    "    concat_layer_3 = Concatenate()([layer_5, layer_6])\n",
    "    # (64, 64, 128)\n",
    "\n",
    "    # final layer\n",
    "    output_layer = Conv2D(filters = 3, kernel_size = (4,4), strides=(1,1), activation='tanh', padding='same')(concat_layer_3)\n",
    "\n",
    "    model = Model([latent_layer,label_layer_1], output_layer)\n",
    "    return model\n",
    "\n",
    "b = define_generator(512)\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:57.866622Z",
     "iopub.status.busy": "2022-08-25T18:59:57.866326Z",
     "iopub.status.idle": "2022-08-25T18:59:58.339781Z",
     "shell.execute_reply": "2022-08-25T18:59:58.338720Z",
     "shell.execute_reply.started": "2022-08-25T18:59:57.866579Z"
    },
    "id": "uOab7yJz55vE",
    "outputId": "181847e5-fbe8-4413-8bce-f0b42e3e12d1"
   },
   "outputs": [],
   "source": [
    "# plot the generator model\n",
    "tf.keras.utils.plot_model(b,to_file='generator.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K74ltFTw63K_"
   },
   "source": [
    "## **Combine** or **GAN** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:58.342324Z",
     "iopub.status.busy": "2022-08-25T18:59:58.341677Z",
     "iopub.status.idle": "2022-08-25T18:59:58.451889Z",
     "shell.execute_reply": "2022-08-25T18:59:58.451016Z",
     "shell.execute_reply.started": "2022-08-25T18:59:58.342281Z"
    },
    "id": "KmspouGiTLhn",
    "outputId": "e8e5585f-27a1-4e26-96c3-cc4a4b40261b"
   },
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model):\n",
    "\n",
    "  d_model.trainable = False\n",
    "\n",
    "  g_latent, g_label = g_model.input\n",
    "  g_output = g_model.output\n",
    "\n",
    "  d_output = d_model([g_output,g_label])\n",
    "\n",
    "  model = Model([g_latent, g_label], d_output)\n",
    "\n",
    "  opt = Adam(learning_rate= 3e-4, beta_1=0.5)\n",
    "  model.compile(loss='mse', optimizer=opt,  metrics=['accuracy'])\n",
    "  \n",
    "  return model\n",
    "c = define_gan(b, a)\n",
    "c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:58.453466Z",
     "iopub.status.busy": "2022-08-25T18:59:58.453169Z",
     "iopub.status.idle": "2022-08-25T18:59:58.975167Z",
     "shell.execute_reply": "2022-08-25T18:59:58.974200Z",
     "shell.execute_reply.started": "2022-08-25T18:59:58.453427Z"
    },
    "id": "Zuaon8Y75-Ps",
    "outputId": "fdef1f99-6ab9-497d-e36c-5f5f76368350"
   },
   "outputs": [],
   "source": [
    "# plot GAN model\n",
    "tf.keras.utils.plot_model(c,to_file='complete_gan.png',show_shapes=True,show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgckUXEo7DxG"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T18:59:58.979395Z",
     "iopub.status.busy": "2022-08-25T18:59:58.978908Z",
     "iopub.status.idle": "2022-08-25T19:00:01.376764Z",
     "shell.execute_reply": "2022-08-25T19:00:01.375979Z",
     "shell.execute_reply.started": "2022-08-25T18:59:58.979357Z"
    },
    "id": "VhzajBRw4wxU"
   },
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "  # load dataset\n",
    "  data = np.load(\"/content/drive/MyDrive/Flower_LSGAN/RGB_64_64_3.npz\")\n",
    "  data = data['a']\n",
    "  data = np.array(data)\n",
    "  shuffle(data)\n",
    "  X = data.astype('float32')\n",
    "  # scale from [0,255] to [-1,1]\n",
    "  X = (X - 127.5) / 127.5\n",
    "  return X\n",
    "k = load_real_samples()\n",
    "print(\"image: \",k.shape)\n",
    "print(\"\\nshape/size of the first 16 data: \",k[:16].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDrk7zuwJoN9"
   },
   "source": [
    "## Plot data with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:01.378839Z",
     "iopub.status.busy": "2022-08-25T19:00:01.377998Z",
     "iopub.status.idle": "2022-08-25T19:00:02.243100Z",
     "shell.execute_reply": "2022-08-25T19:00:02.242362Z",
     "shell.execute_reply.started": "2022-08-25T19:00:01.378800Z"
    },
    "id": "ADMcakmWTN60"
   },
   "outputs": [],
   "source": [
    "def save_plot(x_input,a,n=4):\n",
    "    for i in range(n*n):\n",
    "        plt.subplot(n, n, i+1)\n",
    "        plt.imshow(x_input[i,:,:,:])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Epoch_\"+str(a))\n",
    "    plt.savefig(\"/content/drive/MyDrive/Flower_LSGAN/1/epoch_\"+str(a))\n",
    "    plt.show()\n",
    "# plot data\n",
    "save_plot(k[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcmHiLEK8HQP"
   },
   "source": [
    "## Generate real sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.244427Z",
     "iopub.status.busy": "2022-08-25T19:00:02.244162Z",
     "iopub.status.idle": "2022-08-25T19:00:02.252134Z",
     "shell.execute_reply": "2022-08-25T19:00:02.251504Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.244392Z"
    },
    "id": "uR1tkpInTQq4"
   },
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "\timages = dataset\n",
    "\tix = randint(0, images.shape[0], n_samples)\n",
    "\tX = images[ix]\n",
    "\tz = np.random.randint(0,5,size=(n_samples))\n",
    "\ty = ones((n_samples, 1))\n",
    "\treturn [X,z], y\n",
    "# d = generate_real_samples(k, 32)\n",
    "# print(\"Generate real data as a batch randomly: \",d[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MQHAJ0P8kPF"
   },
   "source": [
    "## Generate latent point function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.254474Z",
     "iopub.status.busy": "2022-08-25T19:00:02.253344Z",
     "iopub.status.idle": "2022-08-25T19:00:02.268491Z",
     "shell.execute_reply": "2022-08-25T19:00:02.267488Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.254435Z"
    },
    "id": "Fv9g2IJXTSpw"
   },
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "  x_input = randn(latent_dim * n_samples)\n",
    "  z_input = x_input.reshape(n_samples, latent_dim)\n",
    "  z = np.random.randint(0,5,size=(n_samples))\n",
    "  return [z_input,z]\n",
    "# p = generate_latent_points(512, 32)\n",
    "# print(\"Generate latent point(with label) as a batch: \",p[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IAEwd9V9GsR"
   },
   "source": [
    "## Generate Fake samples of image with label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.270259Z",
     "iopub.status.busy": "2022-08-25T19:00:02.269864Z",
     "iopub.status.idle": "2022-08-25T19:00:02.282693Z",
     "shell.execute_reply": "2022-08-25T19:00:02.281622Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.270223Z"
    },
    "id": "qTcS-0Ou42nA"
   },
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\tz_input,z = generate_latent_points(latent_dim, n_samples)\n",
    "\timages = generator.predict([z_input,z])\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn [images, z], y\n",
    "# with tf.device(device_name):\n",
    "\t# kh = generate_fake_samples(b, 512, 32)\n",
    "\t# print(\"shape of the generated images: \",kh[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Pb6LIHF9mZM"
   },
   "source": [
    "## Summarize the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.284589Z",
     "iopub.status.busy": "2022-08-25T19:00:02.284227Z",
     "iopub.status.idle": "2022-08-25T19:00:02.296706Z",
     "shell.execute_reply": "2022-08-25T19:00:02.295596Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.284555Z"
    },
    "id": "7c0-Luo746t2"
   },
   "outputs": [],
   "source": [
    "def summarize_the_model(generator, count, latent_dim = 100):\n",
    "    latent_points = generate_latent_points(latent_dim= latent_dim, n_samples= 16)\n",
    "    X  = generator.predict(latent_points)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    save_plot(X, count, n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.298768Z",
     "iopub.status.busy": "2022-08-25T19:00:02.298507Z",
     "iopub.status.idle": "2022-08-25T19:00:02.310858Z",
     "shell.execute_reply": "2022-08-25T19:00:02.310052Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.298736Z"
    },
    "id": "AgkghuoxJMqv"
   },
   "outputs": [],
   "source": [
    "def save_figure(generator,a,latent_dim = 512,n=4):\n",
    "    latent_points, labels = generate_latent_points(latent_dim= latent_dim, n_samples= 16)\n",
    "    X  = generator.predict([latent_points, labels])\n",
    "    # plt.title(\"Epoch_\"+str(a+1),loc = \"center\")\n",
    "    for j in range(n*n):\n",
    "        plt.subplot(n, n, j+1)\n",
    "        plt.imshow(X[j,:,:,:])\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Epoch_\"+str(a+1))\n",
    "    # plt.savefig(\"/content/drive/MyDrive/GAN_New_Approch/4/epoch_\"+str(a+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWKEC8MA9tGE"
   },
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.312831Z",
     "iopub.status.busy": "2022-08-25T19:00:02.312527Z",
     "iopub.status.idle": "2022-08-25T19:00:02.334351Z",
     "shell.execute_reply": "2022-08-25T19:00:02.333238Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.312798Z"
    },
    "id": "VtrEiQruSiOE"
   },
   "outputs": [],
   "source": [
    "def train(g_model, d_model, gan_model, dataset, latent_dim= 100, n_epochs=3, n_batch=128):\n",
    "\n",
    "  print(\"No. of epoch: \",n_epochs)\n",
    "  bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "  print(\"Data Size: \",dataset.shape[0])\n",
    "  print(\"batch per epoch: \", bat_per_epo)\n",
    "  print(\"full batch: \",n_batch)\n",
    "  half_batch = int(n_batch / 2)\n",
    "  print(\"half batch: \", half_batch,'\\n')\n",
    "  print(\"*\"*50,'\\n\\n')\n",
    "\n",
    "  d_loss_real_array,d_loss_fake_array =[],[]\n",
    "  g_loss_array = []\n",
    "    \n",
    "  count = 1\n",
    "  for i in range(n_epochs):\n",
    "    d_loss_r,d_loss_f = 0.0,0.0\n",
    "    g_loss = 0.0\n",
    "    \n",
    "    for j in range(bat_per_epo):\n",
    "\n",
    "      [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "      d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "      d_loss_r += (d_loss1 / half_batch)\n",
    "      # print(\"real_loss\")\n",
    "\n",
    "      [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "      d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "      d_loss_f += (d_loss2 / half_batch)\n",
    "      # print(\"fake_loss\")\n",
    "\n",
    "      [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
    "      y_gan = ones((n_batch, 1))\n",
    "      g_loss1,_ = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "      g_loss += (g_loss1 / n_batch)\n",
    "\n",
    "    d_loss_real_array.append(d_loss_r/bat_per_epo)\n",
    "    d_loss_fake_array.append(d_loss_f/bat_per_epo)\n",
    "    g_loss_array.append(g_loss/bat_per_epo)\n",
    "\n",
    "    print('epoch -> [%d/%d], discriminator_loss_for_real_data = %f, discriminator_loss_for_fake_data = %f, generator_loss = %f' %(i+1, n_epochs, d_loss_r/bat_per_epo, d_loss_f/bat_per_epo, g_loss/bat_per_epo))\n",
    "    if((i+1)%20==0):\n",
    "        summarize_the_model(g_model,count,latent_dim)\n",
    "        count = count + 1\n",
    "    g_model.save(\"/content/drive/MyDrive/Flower_LSGAN/1/generator_model.h5\")\n",
    "    np.savez_compressed('/content/drive/MyDrive/Flower_LSGAN/1/loss_record.npz', a=d_loss_real_array, b=d_loss_fake_array, c=g_loss_array)\n",
    "    # save_figure(g_model,i,latent_dim = 512,n=4)\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "  return d_loss_real_array, d_loss_fake_array, g_loss_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsaXTk2x987F"
   },
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T19:00:02.336651Z",
     "iopub.status.busy": "2022-08-25T19:00:02.336295Z",
     "iopub.status.idle": "2022-08-25T20:41:57.841067Z",
     "shell.execute_reply": "2022-08-25T20:41:57.840269Z",
     "shell.execute_reply.started": "2022-08-25T19:00:02.336605Z"
    },
    "id": "gMtQt5LfTY82",
    "outputId": "e1bf26fc-e7c8-4ddd-b5e8-0b5a4241dba3"
   },
   "outputs": [],
   "source": [
    "with tf.device(device_name):\n",
    "\n",
    "  latent_dim = 512\n",
    "  n_epochs = 200\n",
    "  n_batch = 32\n",
    "  d_model = define_discriminator()\n",
    "  g_model = define_generator(latent_dim)\n",
    "  gan_model = define_gan(g_model, d_model)\n",
    "  dataset = load_real_samples()\n",
    "  print('\\nREADY TO GO !!!\\n')\n",
    "  \n",
    "  d_loss_real_array, d_loss_fake_array, g_loss_array = train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs, n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWkVxCBlxi3p"
   },
   "source": [
    "## Plot Loss Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.843119Z",
     "iopub.status.busy": "2022-08-25T20:41:57.842477Z",
     "iopub.status.idle": "2022-08-25T20:41:57.851323Z",
     "shell.execute_reply": "2022-08-25T20:41:57.850478Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.843080Z"
    },
    "id": "L3aWl3VFrPaZ"
   },
   "outputs": [],
   "source": [
    "# g_model.save(\"/content/sample_data/generator_model_c_dcgan.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.854114Z",
     "iopub.status.busy": "2022-08-25T20:41:57.853859Z",
     "iopub.status.idle": "2022-08-25T20:41:57.869731Z",
     "shell.execute_reply": "2022-08-25T20:41:57.866877Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.854084Z"
    },
    "id": "1EaPyJ2PKh_Y",
    "outputId": "d875b8e0-cab4-4ec6-c213-625cb53958ea"
   },
   "outputs": [],
   "source": [
    "# loaded = np.load('/content/drive/MyDrive/GAN_New_Approch/4/loss_record_4.npz')\n",
    "# print(loaded['a'].shape)\n",
    "# print(loaded['b'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.871384Z",
     "iopub.status.busy": "2022-08-25T20:41:57.871042Z",
     "iopub.status.idle": "2022-08-25T20:41:57.879590Z",
     "shell.execute_reply": "2022-08-25T20:41:57.877879Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.871317Z"
    },
    "id": "DPUEiC7tx4if"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "\n",
    "# d_loss = np.array(loaded['a'])\n",
    "# g_loss = np.array(loaded['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.883375Z",
     "iopub.status.busy": "2022-08-25T20:41:57.882253Z",
     "iopub.status.idle": "2022-08-25T20:41:57.897442Z",
     "shell.execute_reply": "2022-08-25T20:41:57.896644Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.883236Z"
    },
    "id": "zw228YsVuN0O"
   },
   "outputs": [],
   "source": [
    "d_loss = np.array([(i+j) for i,j in zip(d_loss_real_array, d_loss_fake_array)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.899100Z",
     "iopub.status.busy": "2022-08-25T20:41:57.898688Z",
     "iopub.status.idle": "2022-08-25T20:41:57.907204Z",
     "shell.execute_reply": "2022-08-25T20:41:57.906404Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.899066Z"
    },
    "id": "oMK7DxoPuGxL"
   },
   "outputs": [],
   "source": [
    "g_loss = g_loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:57.909351Z",
     "iopub.status.busy": "2022-08-25T20:41:57.908840Z",
     "iopub.status.idle": "2022-08-25T20:41:58.373670Z",
     "shell.execute_reply": "2022-08-25T20:41:58.372994Z",
     "shell.execute_reply.started": "2022-08-25T20:41:57.909291Z"
    },
    "id": "s5qYW8JZxlWt",
    "outputId": "1c26f9b3-8546-4cb9-c03e-06cb7c0101ae"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(d_loss)\n",
    "plt.title('Discriminator Loss Graph')\n",
    "plt.ylabel('Discriminator Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend([\"Discriminator Loss\"], loc='upper right')\n",
    "plt.savefig(\"Discriminator_Loss_Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:58.375368Z",
     "iopub.status.busy": "2022-08-25T20:41:58.374966Z",
     "iopub.status.idle": "2022-08-25T20:41:58.709906Z",
     "shell.execute_reply": "2022-08-25T20:41:58.709185Z",
     "shell.execute_reply.started": "2022-08-25T20:41:58.375315Z"
    },
    "id": "Kla0HPDNymA1",
    "outputId": "249eeedc-10b2-4c67-e3d8-749dd32aa295"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(g_loss,color='orange')\n",
    "plt.title('Generator Loss Graph')\n",
    "plt.ylabel('Generator Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend([\"Generator Loss\"], loc='upper right')\n",
    "plt.savefig(\"Generator_Loss_Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:58.711497Z",
     "iopub.status.busy": "2022-08-25T20:41:58.711241Z",
     "iopub.status.idle": "2022-08-25T20:41:59.032007Z",
     "shell.execute_reply": "2022-08-25T20:41:59.031299Z",
     "shell.execute_reply.started": "2022-08-25T20:41:58.711463Z"
    },
    "id": "Eaf2HX2i0MV3",
    "outputId": "49300bf5-5e84-4476-fd05-83c099b4c24e"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(d_loss)\n",
    "plt.plot(g_loss)\n",
    "plt.title('Loss Graph')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend([\"Discriminator Loss\", \"Generator Loss\"], loc='lower right')\n",
    "plt.savefig(\"Loss_Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzogkr9dAr6c"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:59.033692Z",
     "iopub.status.busy": "2022-08-25T20:41:59.033435Z",
     "iopub.status.idle": "2022-08-25T20:41:59.037575Z",
     "shell.execute_reply": "2022-08-25T20:41:59.036618Z",
     "shell.execute_reply.started": "2022-08-25T20:41:59.033647Z"
    },
    "id": "SGdHt4FStkL5",
    "outputId": "1fd07782-e63e-4874-b3f8-8be1218fe548"
   },
   "outputs": [],
   "source": [
    "# generator = tensorflow.keras.models.load_model(\"/content/drive/MyDrive/GAN_New_Approch/4/generator_model_4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-25T20:41:59.043358Z",
     "iopub.status.busy": "2022-08-25T20:41:59.042632Z",
     "iopub.status.idle": "2022-08-25T20:41:59.048423Z",
     "shell.execute_reply": "2022-08-25T20:41:59.047632Z",
     "shell.execute_reply.started": "2022-08-25T20:41:59.043286Z"
    },
    "id": "3gsO4c2O7iCX",
    "outputId": "4f83404c-3405-4e06-cd36-3bfec88e24ec"
   },
   "outputs": [],
   "source": [
    "# latent_dim = 512\n",
    "# n_samples = 16\n",
    "# z_input, labels = generate_latent_points(latent_dim, n_samples)\n",
    "# print(\"latent points(latent points and labels): \",z_input.shape, labels.shape)\n",
    "# data = [z_input,labels]\n",
    "# pred = generator.predict(data)\n",
    "# # pred = (pred +1 ) / 2.0\n",
    "# print(\"\\nGenerated images with labels: \",pred.shape,'\\n')\n",
    "# save_plot(pred,n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDpQNj8x8IsM"
   },
   "source": [
    "                                              -:END:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZoifA0gJLjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
